<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhengtong Xu</title>

    <meta name="author" content="Zhengtong Xu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zhengtong Xu    徐政通
                </p>
                <p style="text-align: center; font-size: 12px;">
                  Email: <a href="mailto:xu1703@purdue.edu">xu1703 AT purdue.edu</a>
                </p>
                <p>I'm a third-year PhD candidate at <a href="https://www.purdue.edu/">Purdue University</a>, advised by Professor <a href="https://www.purduemars.com/">Yu She</a>. 
                  I was a recipient of the 2025 Magoon Graduate Student Research Excellence Award at Purdue University 
                  (awarded to only 25 PhD students across the entire Purdue College of Engineering).
                </p>
                <p>
                  I received my Bachelor's degree in mechanical engineering at <a href="https://english.hust.edu.cn/"> Huazhong University of Science and Technology</a>.
                </p>
                <p style="text-align:center">
                  <a href="https://scholar.google.com/citations?user=nfwA3RUAAAAJ&hl=en">G. Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/XuZhengtong">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ZhengtongXu">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/zhengtong-xu-4287b8174/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/photo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/photo.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <div style="max-height:200px; overflow-y:auto;">
                    <ul>
                      <li>[Nov. 2024] I have passed my PhD preliminary exam and officially become a PhD candidate</li>
                      <li>[Sep. 2024] One paper accepted by IEEE T-RO</li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  * indicates equal contribution.
                </p>
                <p>
                  My research aims to design learning algorithms for robotic agents, enabling them to perform everyday manipulation tasks with human-level proficiency. To this end, I am currently focusing on  <strong><span style="color:red;">hierarchical multimodal robot learning</span></strong>.
                  Specifically, my research explores: </p>
                <p>
                  <strong>1. Integrating visual, 3D, and tactile modalities to enable multimodal robot learning. </strong><br>
                  <strong>2. Developing interpretable neural-symbolic low-level policies through differentiable optimization.</strong> <br>
                  <strong>3. Deploying pretrained vision-language models for high-level, open-world reasoning and planning. </strong><br>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

            <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/letacmpc_teaser.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping</span>
                  <br>
                    <strong>Zhengtong Xu</strong>, Yu She
                  <br>
                  <em>IEEE Transactions on Robotics (<strong>T-RO</strong>)</em>, 2024
                  <br>
                </p>

                <a href="https://arxiv.org/abs/2403.04934">arXiv</a> /
                <a href="https://drive.google.com/file/d/1rDwg7dA3Wfhhb3rhry0cIfAxGli7WT7k/view">video</a> /
                <a href="https://github.com/ZhengtongXu/LeTac-MPC">code</a> /
                <a href="#" onclick="toggleBibtex(event, 'LeTac'); return false;">bibtex</a>

                <div id="LeTac" style="display:none; position:absolute; background-color:white; border:1px solid #ccc; padding:10px; width:400px; z-index:100; border-radius:15px; overflow:auto; word-wrap:break-word;">
                  <button onclick="copyBibtex(event, 'LeTac-content')" style="float:right; margin-bottom:5px;">Copy</button>
                  <pre id="LeTac-content" style="margin:0;">
                    @article{xu2024letac,
                      author={Xu, Zhengtong and She, Yu},
                      journal={IEEE Transactions on Robotics}, 
                      title={{LeTac-MPC}: Learning Model Predictive Control for Tactile-Reactive Grasping}, 
                      year={2024},
                      volume={40},
                      number={},
                      pages={4376-4395},
                      doi={10.1109/TRO.2024.3463470}
                    }
                  </pre>
                  <a href="#" onclick="toggleBibtex(event, 'LeTac'); return false;">Close</a>
                </div>

                <p></p>
                <p>
                  A generalizable end-to-end tactile-reactive grasping controller with differentiable MPC, combining learning and model-based approaches.  
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/teaser_diffog.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">DiffOG: Differentiable Policy Trajectory Optimization with Generalizability</span>
                  <br>
                    <strong>Zhengtong Xu</strong>, Zichen Miao, Qiang Qiu, Zhe Zhang, Yu She
                  <br>
                  <em>Under Review</em>, 2025
                  <br>
                </p>

                <a href="https://zhengtongxu.github.io/diffog-website/">website</a> /               
                <a href="https://arxiv.org/abs/2504.13807">arXiv</a> /
                <a href="https://drive.google.com/file/d/19vxJWG5CZg6uCrhsZEANto7W33nWLZbl/view?usp=sharing">video</a> /
                <a href="">code(soon)</a> /
                <a href="#" onclick="toggleBibtex(event, 'DiffOG'); return false;">bibtex</a>

                <div id="DiffOG" style="display:none; position:absolute; background-color:white; border:1px solid #ccc; padding:10px; width:400px; z-index:100; border-radius:15px; overflow:auto; word-wrap:break-word;">
                  <button onclick="copyBibtex(event, 'DiffOG-content')" style="float:right; margin-bottom:5px;">Copy</button>
                  <pre id="DiffOG-content" style="margin:0;">
                    @misc{xu2025diffog,
                      title={{DiffOG}: Differentiable Policy Trajectory Optimization with Generalizability}, 
                      author={Zhengtong Xu and Zichen Miao and Qiang Qiu and Zhe Zhang and Yu She},
                      year={2025},
                      eprint={2504.13807},
                      archivePrefix={arXiv},
                      primaryClass={cs.RO},
                      url={https://arxiv.org/abs/2504.13807}, 
                    }
                  </pre>
                  <a href="#" onclick="toggleBibtex(event, 'DiffOG'); return false;">Close</a>
                </div>
                
                <p></p>
                <p>
                  DiffOG introduces a transformer-based differentiable trajectory optimization framework for action refinement in imitation learning. 
                </p>
              </td>
            </tr>


              <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/cp.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">Canonical Policy: Learning Canonical 3D Representation for Equivariant Policy</span>
                  <br>
                    Zhiyuan Zhang*, <strong>Zhengtong Xu*</strong>, Jai Nanda Lakamsani, Yu She
                  <br>
                  <em>Under Review</em>, 2025
                  <br>
                </p>

                <a href="https://zhangzhiyuanzhang.github.io/cp-website/">website</a> /
                <a href="">arXiv(soon)</a> /
                <a href="">video(soon)</a> /
                <a href="">code(soon)</a>
                <p></p>
                <p>
                  Canonical Policy enables equivariant observation-to-action mappings by grouping both in-distribution and out-of-distribution point clouds to a canonical 3D representation.
                </p>
              </td>
            </tr>

              <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/manifeel.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">ManiFeel: Benchmarking and Understanding Visuotactile Manipulation Policy Learning</span>
                  <br>
                    Quan Khanh Luu*, Pokuang Zhou*, <strong>Zhengtong Xu*</strong>, Zhiyuan Zhang, Qiang Qiu, Yu She
                  <br>
                  <em>Under Review</em>, 2025
                  <br>
                </p>

                <a href="">website(soon)</a> /
                <a href="">arXiv(soon)</a> /
                <a href="">video(soon)</a> /
                <a href="">code(soon)</a>
                <p></p>
                <p>
                  ManiFeel is a reproducible and scalable simulation benchmark for studying supervised visuotactile policy learning.
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/unit_teaser.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">UniT: Data Efficient Tactile Representation with Generalization to Unseen Objects</span>
                  <br>
                    <strong>Zhengtong Xu</strong>, Raghava Uppuluri, Xinwei Zhang, Cael Fitch, Philip Glen Crandall, Wan Shou, Dongyi Wang, Yu She
                  <br>
                  <em>IEEE Robotics and Automation Letters (<strong>RA-L</strong>)</em>, 2025
                  <br>
                </p>

                <a href="https://zhengtongxu.github.io/unit-website/">website</a> /
                <a href="https://www.arxiv.org/abs/2408.06481">arXiv</a> /
                <a href="https://www.youtube.com/watch?v=K7dULei0I3Y">video</a> /
                <a href="https://github.com/ZhengtongXu/UniT">code</a> /
                <a href="#" onclick="toggleBibtex(event, 'UniT'); return false;">bibtex</a>

                <div id="UniT" style="display:none; position:absolute; background-color:white; border:1px solid #ccc; padding:10px; width:400px; z-index:100; border-radius:15px; overflow:auto; word-wrap:break-word;">
                  <button onclick="copyBibtex(event, 'UniT-content')" style="float:right; margin-bottom:5px;">Copy</button>
                  <pre id="UniT-content" style="margin:0;">
                    @misc{xu2025unit,
                      title={{UniT}: Data Efficient Tactile Representation with Generalization to Unseen Objects}, 
                      author={Zhengtong Xu and Raghava Uppuluri and Xinwei Zhang and Cael Fitch and Philip Glen Crandall and Wan Shou and Dongyi Wang and Yu She},
                      year={2025},
                      eprint={2408.06481},
                      archivePrefix={arXiv},
                      primaryClass={cs.RO},
                      url={https://arxiv.org/abs/2408.06481}, 
                    }
                  </pre>
                  <a href="#" onclick="toggleBibtex(event, 'UniT'); return false;">Close</a>
                </div>
                
                <p></p>
                <p>
                  Learn a tactile representation with generalizability only by a single simple object.
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/vilp_teaser.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">VILP: Imitation Learning with Latent Video Planning</span>
                  <br>
                    <strong>Zhengtong Xu</strong>, Qiang Qiu, Yu She
                  <br>
                  <em>IEEE Robotics and Automation Letters (<strong>RA-L</strong>)</em>, 2025
                  <br>
                </p>

                <a href="https://arxiv.org/abs/2502.01784">arXiv</a> /
                <a href="https://www.youtube.com/watch?v=sfa_AmI0NoI">video</a> /
                <a href="https://github.com/ZhengtongXu/VILP">code</a> /
                <a href="#" onclick="toggleBibtex(event, 'VILP'); return false;">bibtex</a>

                <div id="VILP" style="display:none; position:absolute; background-color:white; border:1px solid #ccc; padding:10px; width:400px; z-index:100; border-radius:15px; overflow:auto; word-wrap:break-word;">
                  <button onclick="copyBibtex(event, 'VILP-content')" style="float:right; margin-bottom:5px;">Copy</button>
                  <pre id="VILP-content" style="margin:0;">
                    @misc{xu2025vilp,
                      title={VILP: Imitation Learning with Latent Video Planning}, 
                      author={Zhengtong Xu and Qiang Qiu and Yu She},
                      year={2025},
                      eprint={2502.01784},
                      archivePrefix={arXiv},
                      primaryClass={cs.RO},
                      url={https://arxiv.org/abs/2502.01784}, 
                    }
                  </pre>
                  <a href="#" onclick="toggleBibtex(event, 'UniT'); return false;">Close</a>
                </div>
                
                <p></p>
                <p>
                  VILP integrates the video generation model into policies, enabling the representation of multi-modal action distributions while reducing reliance on extensive high-quality robot action data.
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/cbf_hr.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">Safe Human-Robot Collaboration with Risk-tunable Control Barrier Functions</span>
                  <br>
                    Vipul K. Sharma*, Pokuang Zhou*, <strong>Zhengtong Xu*</strong>, Yu She, S. Sivaranjani
                  <br>
                  <em>IEEE/ASME Transactions on Mechatronics (<strong>TMECH</strong>)</em>, 2025
                  <br>
                </p>

                <a href="">arXiv(soon)</a> /
                <a href="">video(soon)</a>
                <p></p>
                <p>
                  We address safety in human-robot collaboration with uncertain human positions by formulating a chance-constrained problem using uncertain control barrier functions.
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/leto_teaser_short.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization</span>
                  <br>
                    <strong>Zhengtong Xu</strong>, Yu She
                  <br>
                  <em>IEEE Transactions on Automation Science and Engineering (<strong>T-ASE</strong>)</em>, 2024
                  <br>
                </p>

                <a href="https://arxiv.org/abs/2401.17500">arXiv</a> /
                <a href="https://drive.google.com/file/d/1-Ty2JRg8COrHM_cl0vaj-xSGzjnZOg7L/view">video</a> /
                <a href="https://github.com/ZhengtongXu/LeTO">code</a> /
                <a href="#" onclick="toggleBibtex(event, 'LeTO'); return false;">bibtex</a>

                <div id="LeTO" style="display:none; position:absolute; background-color:white; border:1px solid #ccc; padding:10px; width:400px; z-index:100; border-radius:15px; overflow:auto; word-wrap:break-word;">
                  <button onclick="copyBibtex(event, 'LeTO-content')" style="float:right; margin-bottom:5px;">Copy</button>
                  <pre id="LeTO-content" style="margin:0;">
                    @article{xu2024leto,
                      title={LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization},
                      author={Xu, Zhengtong and She, Yu},
                      journal={arXiv preprint arXiv:2401.17500},
                      year={2024}
                    }
                  </pre>
                  <a href="#" onclick="toggleBibtex(event, 'LeTO'); return false;">Close</a>
                </div>
                
                <p></p>
                <p>
                  LeTO is a "gray box" method which marries optimization-based safety and interpretability with representational abilities of neural networks.
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
                <img src="images/vistac.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
              </td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">VisTac: Toward a Unified Multimodal Sensing Finger for Robotic Manipulation</span>
                  <br>
                  Sheeraz Athar<sup>*</sup>,  Gaurav Patel<sup>*</sup>, <strong>Zhengtong Xu</strong>, Qiang Qiu, Yu She
                  <br>
                  <em>IEEE Sensors Journal</em>, 2023
                  <br>
                </p>

                <a href="https://ieeexplore.ieee.org/document/10242327">paper</a> /
                <a href="https://www.youtube.com/watch?v=SLN2gQ_haYs">video</a> /
                <a href="#" onclick="toggleBibtex(event, 'Vistac'); return false;">bibtex</a>

                <div id="Vistac" style="display:none; position:absolute; background-color:white; border:1px solid #ccc; padding:10px; width:400px; z-index:100; border-radius:15px; overflow:auto; word-wrap:break-word;">
                  <button onclick="copyBibtex(event, 'Vistac-content')" style="float:right; margin-bottom:5px;">Copy</button>
                  <pre id="Vistac-content" style="margin:0;">
                    @article{athar2023vistac,
                      title={Vistac towards a unified multi-modal sensing finger for robotic manipulation},
                      author={Athar, Sheeraz and Patel, Gaurav and Xu, Zhengtong and Qiu, Qiang and She, Yu},
                      journal={IEEE Sensors Journal},
                      year={2023},
                      publisher={IEEE}
                    }
                  </pre>
                  <a href="#" onclick="toggleBibtex(event, 'Vistac'); return false;">Close</a>
                </div>
                
                <p></p>
                <p>
                  VisTac seamlessly combines high-resolution tactile and visual perception in a single unified device.
                </p>
              </td>
            </tr>

          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Awards</h2>
                  <ul>
                    <li><strong>Magoon Graduate Student Research Excellence Award</strong>, Purdue University, 2025</li>
                    <li><strong>Dr. Theodore J. and Isabel M. Williams Fellowship</strong>, Purdue University, 2022</li>
                    <li><strong>Chinese National Scholarship</strong>,  Ministry of Education of China, 2017</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Reviewer Service</h2>
                  <ul>
                    <li>Conference on Robot Learning <strong>(CoRL)</strong>, 2025</li>
                    <li>IEEE Robotics and Automation Letters <strong>(RA-L)</strong>, 2025</li>
                    <li>IEEE Transactions on Robotics <strong>(T-RO)</strong>, 2024</li>
                    <li>IEEE International Conference on Robotics and Automation <strong>(ICRA)</strong>, 2024</li>
                  </ul>
                </td>
              </tr>
            </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Teaching</h2>
                  <ul>
                    <li><strong>Vertically Integrated Projects (VIP)-GE Robotics and Autonomous Systems</strong>, Grad Mentor, Spring 2024/Fall 2023/Summer 2023</li>
                    <li><strong>IE 474-Industrial Control Systems</strong>,  Teaching Assistant, Fall 2022</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
    <script>
      function toggleBibtex(event, bibtexId) {
        var bibtexDiv = document.getElementById(bibtexId);
        if (bibtexDiv.style.display === 'none') {
          var x = event.pageX;
          var y = event.pageY;
          
          bibtexDiv.style.left = x + 'px';
          bibtexDiv.style.top = y + 'px';
          
          bibtexDiv.style.display = 'block';
        } else {
          bibtexDiv.style.display = 'none';
        }
      }

      function copyBibtex(event, contentId) {
        event.stopPropagation();
        const content = document.getElementById(contentId).textContent;
        const lines = content.split('\n')
          .map(line => line.trim())
          .filter(line => line); // Remove empty lines

        const formattedLines = lines.map((line, index) => {
          if (index === 0) return line;  // First line
          if (line === '}') return line;  // Closing brace
          return '  ' + line;  // All other lines
        });

        const formattedContent = formattedLines.join('\n');

        navigator.clipboard.writeText(formattedContent)
          .then(() => {
            const button = event.target;
            const originalText = button.textContent;
            button.textContent = 'Copied!';
            setTimeout(() => {
              button.textContent = originalText;
            }, 1500);
          })
          .catch(err => {
            console.error('Failed to copy:', err);
          });
      }
    </script>
  </body>
</html>
